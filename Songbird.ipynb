{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tumor Classification in MRI: a Comparision of Neural Network Architectures\n",
    "\n",
    "**Group:** Songbird  \n",
    "**Members:** Charlotte de Vries, Jiazhen Tang, Paulo Zirlis  \n",
    "**Course**: Deep Learning in Python - UvA 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816914ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup block\n",
    "\n",
    "import os\n",
    "import time  # ConvergenceTimer\n",
    "\n",
    "# --- 1. FORCE LEGACY KERAS (CRITICAL FOR ViT) ---\n",
    "# This must run before any other keras/tensorflow imports\n",
    "# To ensure the correct version for the models\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Input, models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from transformers import TFAutoModelForImageClassification\n",
    "from transformers import TFViTModel\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 3. APPLY THE \"INPUT LAYER\" PATCH (CRITICAL FOR CNN) ---\n",
    "# This allows the Keras 2 InputLayer to accept the 'shape' argument\n",
    "# preventing the crash in CNN code.\n",
    "\n",
    "OriginalInputLayer = layers.InputLayer\n",
    "\n",
    "class FriendlyInputLayer(OriginalInputLayer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # If 'shape' is used (Keras 3 style), swap it to 'input_shape' (Keras 2 style)\n",
    "        if 'shape' in kwargs and 'input_shape' not in kwargs:\n",
    "            kwargs['input_shape'] = kwargs.pop('shape')\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "# Overwrite the class in the 'layers' module so code uses it automatically\n",
    "layers.InputLayer = FriendlyInputLayer\n",
    "tf.keras.layers.InputLayer = FriendlyInputLayer\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Setup timer callback\n",
    "class ConvergenceTimer(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.start_time = time.time()\n",
    "        print(\"Training started...\")\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        self.end_time = time.time()\n",
    "        self.total_time = self.end_time - self.start_time\n",
    "        print(\"Training finished.\")\n",
    "        print(f\"Time to converge: {self.total_time:.2f} seconds ({self.total_time/60:.2f} minutes)\")\n",
    "\n",
    "\n",
    "print(\"Setup OK: Legacy mode enabled & InputLayer patched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Introduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Medical imaging has been a prominent field for machine learning techniques in recent years. With the rise of Deep Learning models, such complex tasks have increasingly been tackled by researchers as the techniques become more advances and refined over the years. As novice data scientists, we were curious about the trade-off involved in different model architectures, especially the complexity of their networks, the accuracy they can achieve and the time they take to train and fine-tune. With that in mind, our group deciced to test different deep learning architectures to the visual classification task of identifying brain tumors in MRI images and compare them based on accuracy and time to converge. This work may be helpful to researchers and health professionals who question the applicability and required complexity of these models for image classification in the health industry. As individuals, we hope this projects enhances our data science skills and delve more into the field.\n",
    "\n",
    "<br><br>\n",
    "**STILL NEED TO REFINE A BIT MORE!!!!**\n",
    "<br><br>\n",
    "\n",
    "We wished to implement and compare the following NN architectures:\n",
    "- Custom Convolutional Neural Network (CNN)\n",
    "- Custom Residual Network\n",
    "- Pre-trained Residual Network (ResNet50)\n",
    "- Pre-trained Residual Network (ResNet50) with fine-tuning\n",
    "- Pre-trained Visual Transformer (ViT)\n",
    "- Pre-trained Visual Transformer (ViT) with fine-tuning\n",
    "\n",
    "<br>\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Introduction\n",
    "2. Methods\n",
    "    - Data\n",
    "    - Custom Convolutional Neural Network (CNN)\n",
    "    - Custom Residual Network\n",
    "    - Pre-trained Residual Network (ResNet50)\n",
    "    - Pre-trained Residual Network (ResNet50) with fine-tuning\n",
    "    - Pre-trained Visual Transformer (ViT)\n",
    "    - Pre-trained Visual Transformer (ViT) with fine-tuning\n",
    "3. Results\n",
    "    - Compile Results\n",
    "    - Comparison Plots\n",
    "    - Model Insights\n",
    "4. Discussion\n",
    "5. References\n",
    "6. Division of Labour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 2.1 Data\n",
    "\n",
    "The dataset includes high-resolution CT and MRI images captured from multiple patients, with each image labeled with the corresponding tumor type (e.g., glioma, meningioma, etc.). For this project we will focus solely on the **MRI** images for simplicity. The dataset's creator collected these data from different articles to assist researchers and healthcare professionals in developing AI models for the automatic detection, classification, and segmentation of brain tumors. The images were originally organized in two folders: Healthy images and Tumor images; The specific tumor classes were specified in the file names.\n",
    "\n",
    "The classes sizes are as follows:\n",
    "- Healty images: 2000\n",
    "- Tumor images: 3000\n",
    "    - Meningioma: 1112\n",
    "    - Glioma: 672\n",
    "    - Pituitary: 629\n",
    "    - Tumor: 587\n",
    "- **Total of images:** 5000\n",
    "\n",
    "Source: [Brain tumor multimodal image (Kaggle)](https://www.kaggle.com/datasets/murtozalikhon/brain-tumor-multimodal-image-ct-and-mri/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### 2.1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "\n",
    "# Setup paths\n",
    "dataset_path = 'Data/Brain Tumor MRI images'\n",
    "\n",
    "print(f\"Checking contents of: {dataset_path}\")\n",
    "try:\n",
    "    items = os.listdir(dataset_path)\n",
    "    print(\"Found these items:\", items)\n",
    "except: print(\"Error: The dataset_path does not exist.\")\n",
    "\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "\n",
    "# Get list of all folders in the main directory\n",
    "all_items = os.listdir(dataset_path)\n",
    "\n",
    "for item in all_items:\n",
    "    item_path = os.path.join(dataset_path, item)\n",
    "    \n",
    "    # Select only folders (directory)\n",
    "    if os.path.isdir(item_path):\n",
    "        \n",
    "        # --- CASE A: The 'Healthy' Folder ---\n",
    "        if 'healthy' in item.lower():\n",
    "            print(f\"Processing Healthy folder: {item}\")\n",
    "            for filename in os.listdir(item_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):\n",
    "                    filepaths.append(os.path.join(item_path, filename))\n",
    "                    labels.append('Healthy')\n",
    "                    \n",
    "        # --- CASE B: The 'Tumour' Folder (Anything that isn't Healthy) ---\n",
    "        else:\n",
    "            print(f\"Processing Tumour folder: {item}\")\n",
    "            for filename in os.listdir(item_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):\n",
    "                    full_path = os.path.join(item_path, filename)\n",
    "                    name_lower = filename.lower()\n",
    "                    \n",
    "                    # Determine Subtype based on filename\n",
    "                    if 'glioma' in name_lower:\n",
    "                        label = 'Glioma'\n",
    "                    elif 'meningioma' in name_lower:\n",
    "                        label = 'Meningioma'\n",
    "                    elif 'pituitary' in name_lower:\n",
    "                        label = 'Pituitary'\n",
    "                    else:\n",
    "                        label = 'Tumor (Unspecified)' \n",
    "                    \n",
    "                    filepaths.append(full_path)\n",
    "                    labels.append(label)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "\n",
    "# Check results\n",
    "print(f\"Total images found: {len(df)}\")\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### 2.1.2 Oversampling\n",
    "\n",
    "Since the healthy class had almost double the observations than most of the other classes, we performed an oversampling technique to make all classes achieve a size of 2000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify the majority class count\n",
    "max_count = df['label'].value_counts().max()\n",
    "print(f\"Target count per class: {max_count}\")\n",
    "\n",
    "# 2. Separate the dataframe by class\n",
    "groups = df.groupby('label')\n",
    "\n",
    "# 3. Create a list to hold the balanced dataframes\n",
    "balanced_dfs = []\n",
    "\n",
    "for label, group_df in groups:\n",
    "    # If this group is smaller than the max, oversample it\n",
    "    if len(group_df) < max_count:\n",
    "        \n",
    "        # Resample logic:\n",
    "        # replace=True: This allows duplication (essential for oversampling)\n",
    "        # n_samples=max_count: Target number of samples\n",
    "        oversampled_group = resample(group_df, \n",
    "                                     replace=True, \n",
    "                                     n_samples=max_count, \n",
    "                                     random_state=42)\n",
    "        balanced_dfs.append(oversampled_group)\n",
    "        print(f\"Oversampled {label} from {len(group_df)} to {len(oversampled_group)}\")\n",
    "        \n",
    "    else:\n",
    "        # If it's the majority class (Healthy), just keep it as is\n",
    "        balanced_dfs.append(group_df)\n",
    "        print(f\"Kept {label} at {len(group_df)}\")\n",
    "\n",
    "# 4. Concatenate all back into one DataFrame\n",
    "df_balanced = pd.concat(balanced_dfs).reset_index(drop=True)\n",
    "\n",
    "# 5. Shuffle the dataset so classes aren't clustered together\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 6. Verify the new counts\n",
    "print(\"\\nNew Label Distribution:\")\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "# 7. Update your main df variable\n",
    "df = df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### 2.1.3 Train-test split\n",
    "\n",
    "For the split, we determined 70% for training, 20% for the test and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 80% Train, 20% Test (using stratify to keep classes balanced)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Split Train again to get Validation set (e.g. 10% of total with stratify)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.125, shuffle=True, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Val size:   {len(val_df)}\")\n",
    "print(f\"Test size:  {len(test_df)}\")\n",
    "\n",
    "\n",
    "# Visualize to confirm labels are correct\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Random sample to check\n",
    "sample_df = df.sample(10)\n",
    "\n",
    "for i, (index, row) in enumerate(sample_df.iterrows()):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    img = mpimg.imread(row['filepath'])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"{row['label']}\\n{os.path.basename(row['filepath'])[:10]}...\", fontsize=9) # Show label + part of filename\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### 2.1.4 CNN Keras Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Images to correct format for CNN\n",
    "\n",
    "# Define image size and batch size\n",
    "IMG_SIZE = (256, 256) \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create ImageDataGenerators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen   = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Build generators FROM DATAFRAMES\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### 2.1.5 Custom ResNet Keras Generators\n",
    "\n",
    "For faster training and less overfitting the inputsize is (128, 128, 1) for the Residual Networks, so we use the same data frames but with a different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Images to correct input size for ResNet\n",
    "\n",
    "# 1. Define image size and batch size\n",
    "IMG_SIZE = (128, 128) \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 2. Create ImageDataGenerators\n",
    "train_rn_scratch_ = ImageDataGenerator(rescale=1./255)\n",
    "val_rn_scratch_  = ImageDataGenerator(rescale=1./255)\n",
    "test_rn_scratch_  = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 3. Build generators FROM DATAFRAMES\n",
    "train_rn_scratch = train_rn_scratch_.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_rn_scratch = val_rn_scratch_.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_rn_scratch = test_rn_scratch_.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### 2.1.6 ResNet50 Keras Generators\n",
    "\n",
    "For the pretrained ResNet-50 model the input should be the same as the ImageNet which is: (224, 224, 3). Therefore we preprocess the data again so that it matches the input the pretrained model expects (ImageNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### image format same as ImageNet\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_rn_pretrained_ = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "val_rn_pretrained_ = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_rn_pretrained_ = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_rn_pretrained = train_rn_pretrained_.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_rn_pretrained = val_rn_pretrained_.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_rn_pretrained = test_rn_pretrained_.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### 2.1.7 ViT Keras Generators\n",
    "\n",
    "ViT requires a specific image format because of the pretrained base, so we also create separate generators with 'channels_first'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Generators with 'channels_first'\n",
    "# We add data_format='channels_first' to match the Hugging Face ViT requirements\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    data_format='channels_first'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    data_format='channels_first'\n",
    ")\n",
    "\n",
    "\n",
    "# Flow from DataFrame\n",
    "train_vit = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_vit = test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_vit = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,  \n",
    "    data_format='channels_first'\n",
    ")\n",
    "\n",
    "print(\"Generators recreated with Channels First format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 2.2 Custom Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The Convolutional Neural Network (CNN) model designed for this project consists of six convolutional blocks followed by a final block with pooling and fully connected layers. Convolutional blocks 1 through 5 have two instances of convolution layer, batch normalization and activation function (ReLU), followed by max pooling, while the sixth blok has only one instance of convolutional layer, batch nromalization and activation before pooling. Early stopping was added to control for overfitting and underfitting. Batch normalization was used to improve training speed and stability. Regular dropout produces problems when working with images due to the random dropping of pixels which affects the relations of adjacent pixels, so we used Spatial dropout which takes those proximities into account while reducing overfitting. The model was compiled with the Adam optimizer, categorical cross-entropy loss function, and accuracy as the evaluation metric. Additionally, a learning rate scheduler was used to allow for the model to learn quicker at the start and then slowly stabilize better once the loss stops improving. This along with Spatial dropout proved to be very effective in training.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 2.2.1 Architecture\n",
    "\n",
    "**Input and Data Augmentation**\n",
    "- Input layer: shape (256, 256, 1);\n",
    "- Batchsize: 32;\n",
    "- Data Augmentation: Random rotations and horizontal flips;\n",
    "\n",
    "**1st to 4th Convolutional Blocks**\n",
    "- Two 3×3 convolutional layers with Batch Normalization and ReLU;\n",
    "- Max pooling with kernel 2 and stride 2 for downsampling;\n",
    "- 1st block has 16 filters, and this doubles with each block (32, 64, 128);\n",
    "\n",
    "**5th Convolutional Block**\n",
    "- Two 3×3 convolutional layers with 256 filters, Batch Normalization and ReLU;\n",
    "- Spatial Dropout 2D of 20%;\n",
    "- Max pooling with kernel 2 and stride 2 for downsampling;\n",
    "\n",
    "**6th Convolutional Block**\n",
    "- One 3×3 convolutional layers with 512 filters, Batch Normalization and ReLU;\n",
    "- Spatial Dropout 2D of 20%;\n",
    "- Max pooling with kernel 2 and stride 2 for downsampling;\n",
    "\n",
    "**Classification Head**\n",
    "- Global Average Pooling layer;\n",
    "- Fully connected layer with 128 units;\n",
    "- Fully connected layer with 64 units;\n",
    "- Fully connected layer with Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN Architecture\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Custom CNN\n",
    "CNN = keras.Sequential([\n",
    "    \n",
    "    # Input\n",
    "    layers.InputLayer(shape = [256, 256, 1]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    layers.RandomFlip(\"horizontal\"), # flip images horizontally\n",
    "    layers.RandomRotation(0.1),      # rotate images randomly by 10%\n",
    "\n",
    "\n",
    "    # 1st Convolutional Block\n",
    "    layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # 2nd Convolutional Block\n",
    "    layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # 3rd Convolutional Block\n",
    "    layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # 4th Convolutional Block\n",
    "    layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # 5th Convolutional Block\n",
    "    layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.SpatialDropout2D(0.2),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # 6th Convolutional Block\n",
    "    layers.Conv2D(filters=512, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.SpatialDropout2D(0.2),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Classifier Head\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(units=128),\n",
    "    layers.Dense(units=64),\n",
    "    layers.Dense(units=5, activation='softmax')  # 5 classes\n",
    "])\n",
    "\n",
    "CNN.summary()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "CNN.compile(\n",
    "    optimizer = Adam(0.001),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### 2.2.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============== Callbacks ==============\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 15,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# Reduce LR on plateau\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,       # Reduce LR by 80% when stuck\n",
    "    patience=5,       # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6,      \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_CNN.keras',    # Naming the file\n",
    "    monitor='val_accuracy',       # What to monitor\n",
    "    mode='max',                   # 'max' for accuracy, 'min' for loss\n",
    "    save_best_only=True,          \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Timer\n",
    "timer_cnn = ConvergenceTimer()\n",
    "\n",
    "\n",
    "\n",
    "### ============== Training ==============\n",
    "\n",
    "# Fit the model\n",
    "hist_CNN = CNN.fit(\n",
    "    train_gen,\n",
    "    validation_data = val_gen,\n",
    "    batch_size = 32,\n",
    "    epochs = 30,\n",
    "    callbacks = [early_stopping, lr_scheduler, checkpoint, timer_cnn],\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### 2.2.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ============== Evaluation ==============\n",
    "\n",
    "# Define function for train loss and accuracy\n",
    "def eval_train(hist):\n",
    "    # Loss\n",
    "    plt.plot(hist.history['loss'], label='train_loss')\n",
    "    plt.plot(hist.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
    "    plt.plot(hist.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate\n",
    "eval_train(hist_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### 2.2.4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy_CNN = CNN.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 2.3 Custom Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### 2.3.1 Architecture\n",
    "\n",
    "**Input and Data Augmentation**\n",
    "- Input layer: shape (128, 128, 1)\n",
    "- Batchsize: 32\n",
    "- Data Augmentation: Random rotations and horizontal flips\n",
    "\n",
    "**ResNet Block (function)**\n",
    "- Two 3×3 convolutional layers with Batch Normalization and ReLU\n",
    "- Optional 1×1 convolution in the shortcut path to match dimensions\n",
    "- Residual (skip) connection followed by ReLU activation\n",
    "\n",
    "**Backbone Architecture**\n",
    "- Initial convolutional layer followed by multiple residual blocks\n",
    "- The number of filters increases across stages (16 → 32 → 64)\n",
    "- Downsampling is performed by using a stride of 2 in the first block of each stage\n",
    "\n",
    "**Classification Head**\n",
    "- Global Average Pooling layer\n",
    "- Fully connected layer with Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =============== RESNET BLOCK ===============\n",
    "\n",
    "def resnet_block(x, filters, stride=1):\n",
    "    shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, strides=stride, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, 1, strides=stride, padding=\"same\")(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to build the model\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    data_augmentation = keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "    ])\n",
    "\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    x = layers.Conv2D(16, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = resnet_block(x, 16)\n",
    "    x = resnet_block(x, 16)\n",
    "\n",
    "    x = resnet_block(x, 32, stride=2)\n",
    "    x = resnet_block(x, 32)\n",
    "\n",
    "    x = resnet_block(x, 64, stride=2)\n",
    "    x = resnet_block(x, 64)\n",
    "\n",
    "    # Resulted in overfitting on the traindata so left it out\n",
    "    # x = resnet_block(x, 128, stride=2)\n",
    "    # x = resnet_block(x, 128)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =============== MODEL INPUTS ===============\n",
    "\n",
    "num_classes= 5\n",
    "\n",
    "model_rn_scratch = build_resnet(\n",
    "    input_shape=(128, 128, 1),\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### 2.3.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =================== CALLBACKS ===================\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping_rn = EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 8,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_rn_scratch.keras',    # Naming the file\n",
    "    monitor='val_accuracy',       # What to monitor\n",
    "    mode='max',                   # 'max' for accuracy, 'min' for loss\n",
    "    save_best_only=True,          \n",
    "    verbose=0                        # Print a message when saving\n",
    ")\n",
    "\n",
    "# Reduce learning rate when needed\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',    \n",
    "    factor=0.5,            # multiply learning rate with 0.5 if there is no enhancement\n",
    "    patience=3,            \n",
    "    verbose=1,            \n",
    "    min_lr=1e-6            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "### =================== TRAINING ===================\n",
    "\n",
    "## Compile model\n",
    "model_rn_scratch.compile(\n",
    "    optimizer=Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "## Train model\n",
    "timer_rn_scratch = ConvergenceTimer()\n",
    "\n",
    "history_rn_scratch = model_rn_scratch.fit(\n",
    "    train_rn_scratch,\n",
    "    validation_data=val_rn_scratch,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint, early_stopping_rn, reduce_lr, timer_rn_scratch],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### 2.3.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train(history_rn_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### 2.3.4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy_rn_scratch = model_rn_scratch.evaluate(test_rn_scratch)\n",
    "\n",
    "## Extract the best of the trainset\n",
    "# and the corresponding accuracy of the validationset\n",
    "train_accs = history_rn_scratch.history['accuracy']\n",
    "val_accs   = history_rn_scratch.history['val_accuracy']\n",
    "best_train_epoch = train_accs.index(max(train_accs))\n",
    "val_acc_at_best_train = val_accs[best_train_epoch]\n",
    "\n",
    "print(f\"Best train accuracy of the best model: {train_accs[best_train_epoch]:.4f}\")\n",
    "print(f\"Validation accuracy of the best model: {val_acc_at_best_train:.4f}\")\n",
    "print(f\"Test accuracy of the best model: {accuracy_rn_scratch:.4f}\")\n",
    "print(f\"Epoch: {best_train_epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "**Conclusion:** After 50 epochs, the model converged, achieving an accuracy of 0.91 on the training set, 0.87 on the validation set, and 0.88 on the test set. This suggests a slight overfitting to the training data. However, the relatively small gap between training, validation, and test accuracies indicates that the model generalizes reasonably well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### 2.4 Pretrained ResNet **without finetuning**\n",
    "\n",
    "#### 2.4.1 Architecture\n",
    "\n",
    "**Input and Data Augmentation**\n",
    "- Input layer: shape (224, 224, 3)\n",
    "- Batchsize: 32\n",
    "- Data Augmentation: Random rotations and horizontal flips\n",
    "\n",
    "**Backbone Architecture**\n",
    "- Pretrained base from the ResNet-50, trained on ImageNet\n",
    "\n",
    "**Classification Head**\n",
    "- Global Average Pooling layer\n",
    "- Fully connected layer with Softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "## pretrained base\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False   # Freeze weights\n",
    "\n",
    "## attach head\n",
    "model_rn_pt = models.Sequential([\n",
    "\n",
    "    layers.RandomFlip(\"horizontal\"), # data augmentation\n",
    "    layers.RandomRotation(0.1),\n",
    "\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### 2.4.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Callbacks\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_rn_pt.keras',    # Naming the file\n",
    "    monitor='val_accuracy',       # What to monitor\n",
    "    mode='max',                   # 'max' for accuracy, 'min' for loss\n",
    "    save_best_only=True,          \n",
    "    verbose=0                     # Print a message when saving\n",
    ")\n",
    "\n",
    "## Compile model\n",
    "model_rn_pt.compile(\n",
    "    optimizer=Adam(1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "## Train model\n",
    "timer_rn_pt = ConvergenceTimer() # keep track of training time\n",
    "\n",
    "history_rn_pt = model_rn_pt.fit(\n",
    "    train_rn_pretrained,\n",
    "    validation_data=val_rn_pretrained,\n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint, reduce_lr, early_stopping, timer_rn_pt],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "#### 2.4.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train(history_rn_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### 2.4.4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the best model on th test dat\n",
    "_, accuracy_rn_pt = model_rn_pt.evaluate(test_rn_pretrained)\n",
    "\n",
    "## Extract the accuracy of the best model on the trainset\n",
    "# and the corresponding accuracy of the validationset\n",
    "train_accs = history_rn_pt.history['accuracy']\n",
    "val_accs   = history_rn_pt.history['val_accuracy']\n",
    "best_train_epoch = train_accs.index(max(train_accs))\n",
    "val_acc_at_best_train = val_accs[best_train_epoch]\n",
    "\n",
    "print(f\"Best train accuracy of the best model: {train_accs[best_train_epoch]:.4f}\")\n",
    "print(f\"Validation accuracy of the best model: {val_acc_at_best_train:.4f}\")\n",
    "print(f\"Test accuracy of the best model: {accuracy_rn_pt:.4f}\")\n",
    "print(f\"Epoch: {best_train_epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "**Conclusion:** The model has an accuracy of 0.91 on the training set, 0.87 on the validation set, and 0.88 on the test set. This indicates a slight degree of overfitting to the training data. However, the relatively small gap between training, validation, and test performance suggests that the model generalizes reasonably well to unseen data. The model performs similar as the custom ResNet, but the pretrained model converged at epoch 37 instead of 50. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### 2.5 Pretrained ResNet **with finetuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### 2.5.1 Architecture\n",
    "\n",
    "**Input and Data Augmentation**\n",
    "- Input layer: shape (224, 224, 3)\n",
    "- Batchsize: 32\n",
    "- Data Augmentation: Random rotations and horizontal flips\n",
    "\n",
    "**Backbone Architecture**\n",
    "- Pretrained base from the ResNet-50, trained on ImageNet\n",
    "- The first 150 layers of the base are froozen\n",
    "- The last 27 layers of the base are unfroozen, so their weights can be trained on our data\n",
    "\n",
    "**Classification Head**\n",
    "- Global Average Pooling layer\n",
    "- Fully connected layer with Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first 150 layers \"freeze\"\n",
    "for layer in base_model.layers[:150]:\n",
    "    layer.trainable = False\n",
    "\n",
    "## attach head\n",
    "model_rn_pt_finetuned = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Here we unfreeze the last 37 of 177 layers of the ResNet50 so that the weights of these layers can be trained. The first 140 layers stay froozen (untrained).\n",
    "\n",
    "#### 2.5.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Callbacks\n",
    "# Checkpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_rn_pt_finetuned.keras',    # Naming the file\n",
    "    monitor='val_accuracy',       # What to monitor\n",
    "    mode='max',                   # 'max' for accuracy, 'min' for loss\n",
    "    save_best_only=True,          \n",
    "    verbose=0                     # Print a message when saving\n",
    ")\n",
    "\n",
    "## Compile model\n",
    "model_rn_pt_finetuned.compile(\n",
    "    optimizer=Adam(1e-5),    \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "## Train model\n",
    "timer_rn_pt_finetuned = ConvergenceTimer()\n",
    "\n",
    "history_rn_pt_finetuned = model_rn_pt_finetuned.fit(\n",
    "    train_rn_pretrained,\n",
    "    validation_data=val_rn_pretrained,\n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint, early_stopping_rn, reduce_lr, timer_rn_pt_finetuned],\n",
    "    verbose=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### 2.5.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train(history_rn_pt_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "#### 2.5.4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the best model on the testset\n",
    "_, accuracy_rn_pt_finetuned = model_rn_pt_finetuned.evaluate(test_rn_pretrained)\n",
    "\n",
    "## Extract the accuracy of the best model on the trainset\n",
    "# and the corresponding accuracy of the validationset\n",
    "train_accs = history_rn_pt_finetuned.history['accuracy']\n",
    "val_accs   = history_rn_pt_finetuned.history['val_accuracy']\n",
    "best_train_epoch = train_accs.index(max(train_accs))\n",
    "val_acc_at_best_train = val_accs[best_train_epoch]\n",
    "\n",
    "print(f\"Best train accuracy of the best model: {train_accs[best_train_epoch]:.4f}\")\n",
    "print(f\"Validation accuracy of the best model: {val_acc_at_best_train:.4f}\")\n",
    "print(f\"Test accuracy of the best model: {accuracy_rn_pt_finetuned:.4f}\")\n",
    "print(f\"Epoch: {best_train_epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "**Conclusion:** After 13 epochs, the model converged, achieving an accuracy of 0.97 on the training set, 0.95 on the validation set, and 0.94 on the test set. This indicates a slight degree of overfitting to the training data. However, the relatively small gap between training, validation, and test performance suggests that the model generalizes reasonably well to unseen data. The model has the highest accuracy compared to the custom ResNet and pretrained ResNet without finetung indicating that fine tuning enhances the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### 2.6 Pre-trained Vision Transformer **without finetuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### 2.6.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576b434",
   "metadata": {},
   "source": [
    "**Input and Data Augmentation**\n",
    "\n",
    "- Input layer: Shape (224, 224, 3) (implied by the model ID vit-base-patch16-224)\n",
    "- Batchsize: 32 (Standard assumption based on the previous context, though not explicitly in this snippet)\n",
    "- Data Augmentation: Random rotations and horizontal flips (Standard assumption based on the previous context)\n",
    "\n",
    "**Backbone Architecture** \n",
    "- Pretrained base: Vision Transformer (ViT) Base model (google/vit-base-patch16-224)\n",
    "- Patching: Images are split into 16x16 patches\n",
    "- Parameters: ~86 Million parameters\n",
    "- Source: Pre-trained on ImageNet-21k (implied by the Hugging Face model ID)\n",
    "\n",
    "**Classification Head**\n",
    "- Output Layer: Linear projection layer adjusted for num_classes (5 classes)\n",
    "- Activation: Softmax (implicitly handled via from_logits = True in the loss function during training)\n",
    "- Optimizer: Adam with a learning rate of 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the number of classes (should be 5)\n",
    "num_classes = len(train_vit.class_indices)\n",
    "\n",
    "# Load the ViT Base Model (86M Parameters)\n",
    "vit_id = \"google/vit-base-patch16-224\"\n",
    "\n",
    "vit = TFAutoModelForImageClassification.from_pretrained(\n",
    "    vit_id, \n",
    "    num_labels=num_classes, \n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=False\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "# jit_compile=True will help speed up this heavy model on the GPU\n",
    "vit.compile(\n",
    "    optimizer = Adam(learning_rate=5e-5), # ViT requires a small learning rate (5e-5)\n",
    "    loss = CategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy'],\n",
    "    jit_compile=True \n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded and compiled {vit_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### 2.6.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ================ CALLBACKS ================\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "timer_vit = ConvergenceTimer()\n",
    "\n",
    "\n",
    "### ================ TRAIN ================\n",
    "\n",
    "history_vit = vit.fit(\n",
    "    train_vit,\n",
    "    epochs=10, \n",
    "    validation_data=val_vit,\n",
    "    callbacks=[early_stopping, timer_vit]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "#### 2.6.3 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train(history_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### 2.6.4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy_vit = vit.evaluate(test_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98570d6",
   "metadata": {},
   "source": [
    "Conclusion: The model has an accuracy of 0.93 on the training set and 0.91 on the validation set (at the best epoch). This indicates a slight degree of overfitting to the training data, as the training loss continues to decrease while the validation loss fluctuates. However, the gap remains small, suggesting the model generalizes well. Notably, the pretrained model achieved high performance very quickly, reaching ~90% validation accuracy by just the second epoch and converging in less (11) epochs. However, its running time is the longest among all models (36.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### 2.7 Pre-trained ViT **with finetuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d286eb",
   "metadata": {},
   "source": [
    "#### 2.7.1 Preparations\n",
    "**Stage 1 (Frozen)**\n",
    "\n",
    "- Action: The ViT backbone is set to trainable = False. A custom classification head (Dense 512 $\\to$ Dropout $\\to$ Dense 256 $\\to$ Dropout $\\to$ Output) is added. The model is compiled with a standard learning rate (1e-3) and trained for 5 epochs.\n",
    "\n",
    "- Goal: To force the new, randomly initialized \"Custom Head\" layers to stabilize and learn basic patterns without disturbing the pre-trained weights of the ViT Backbone. This prevents \"catastrophic forgetting\" of the features the backbone already knows.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Stage 2 (Unfrozen)** \n",
    "\n",
    "- Action: The backbone is set to trainable = True. The model is re-compiled with a much lower learning rate (1e-5) to ensure gentle updates. It is then trained for up to 10 more epochs, utilizing Early Stopping to prevent overfitting.\n",
    "\n",
    "- Goal: Now that the head is stable, we let the whole network (Backbone + Head) train together. The low learning rate allows the backbone to \"fine-tune\" its features specifically for this dataset, perfecting the results without destroying the pre-trained knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global timer for 2 stages\n",
    "class CumulativeTimer(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.total_time = 0.0\n",
    "        self.stage_start_time = 0.0\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.stage_start_time = time.time()\n",
    "        print(\"Starting training stage...\")\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        end_time = time.time()\n",
    "        duration = end_time - self.stage_start_time\n",
    "        self.total_time += duration\n",
    "        print(f\"Stage finished in {duration:.2f} seconds.\")\n",
    "        print(f\"Total Cumulative Time so far: {self.total_time:.2f} seconds ({self.total_time/60:.2f} minutes)\")\n",
    "\n",
    "# Instantiate ONE timer object to re-use across both .fit() calls\n",
    "global_timer = CumulativeTimer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "#### 2.7.2 Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP BACKBONE\n",
    "backbone = TFViTModel.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    use_safetensors=False\n",
    ")\n",
    "backbone.trainable = False # Freeze the backbone\n",
    "\n",
    "# 2. BUILD MODEL (Same architecture as before)\n",
    "inputs = Input(shape=(3, 224, 224), name=\"input_image\")\n",
    "\n",
    "def get_vit_features(x):\n",
    "    return backbone(x).last_hidden_state\n",
    "\n",
    "x = layers.Lambda(get_vit_features)(inputs)\n",
    "x = x[:, 0, :] # CLS token\n",
    "\n",
    "# Custom Head\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, name=\"prediction_head\")(x)\n",
    "\n",
    "model_two_stage = Model(inputs=inputs, outputs=outputs, name=\"ViT_Two_Stage\")\n",
    "\n",
    "\n",
    "# Compile\n",
    "model_two_stage.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),   # Standard learning rate to stabilize head\n",
    "    loss=CategoricalCrossentropy(from_logits=True), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "# ==================== TRAIN ====================\n",
    "print(\"\\n--- STAGE 1: Training Head Only (Frozen Backbone) ---\")\n",
    "\n",
    "train_steps = len(train_df) // 32\n",
    "val_steps = len(val_df) // 32\n",
    "\n",
    "history_stage1 = model_two_stage.fit(\n",
    "    train_vit,\n",
    "    epochs=5,  \n",
    "    validation_data=val_vit,\n",
    "    batch_size = 32,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=val_steps,\n",
    "    # No early stopping needed here\n",
    "    callbacks=[global_timer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "#### 2.7.3 Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze backbone\n",
    "backbone.trainable = True\n",
    "print(\"\\nBackbone unfrozen. The model now has\", model_two_stage.count_params(), \"trainable parameters.\")\n",
    "\n",
    "# Re-compile\n",
    "model_two_stage.compile(\n",
    "    optimizer = Adam(learning_rate=1e-5), # low LR to avoid breaking pre-trained weights\n",
    "    loss = CategoricalCrossentropy(from_logits=True), \n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Early Stopping to prevent overfitting the backbone\n",
    "early_stopping_stage2 = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# ==================== TRAIN ====================\n",
    "history_stage2 = model_two_stage.fit(\n",
    "    train_vit,\n",
    "    epochs = 10,\n",
    "    validation_data = val_vit,\n",
    "    steps_per_epoch = train_steps,\n",
    "    validation_steps = val_steps,\n",
    "    callbacks = [early_stopping_stage2,global_timer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "#### 2.7.4 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train(history_stage2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "#### 2.7.4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy_vit_finetuned = model_two_stage.evaluate(test_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdacfcce",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "The two-stage training approach successfully stabilized the model before fine-tuning, resulting in consistent performance gains. In Stage 1 (Frozen Backbone), the model quickly learned the new classification task, jumping from 54% to 81% accuracy in just 5 epochs. This confirmed that the custom head was successfully initialized without distorting the pre-trained features.\n",
    "\n",
    "In Stage 2 (Fine-Tuning), unfreezing the backbone and applying a low learning rate further improved performance. The model refined its accuracy from ~82% to 86.7% on the training set and 86.1% on the validation set. The final validation loss (0.33) and accuracy remained very close to the training metrics, indicating that the model generalizes well with minimal overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### 3.2 Compile results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DF with all accuracies and times\n",
    "\n",
    "all_models = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Custom CNN', \n",
    "        'Custom ResNet', \n",
    "        'Pretrained ResNet', \n",
    "        'Fine-tuned ResNet', \n",
    "        'Pretrained ViT', \n",
    "        'Fine-tuned ViT'\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        accuracy_CNN, \n",
    "        accuracy_rn_scratch, \n",
    "        accuracy_rn_pt, \n",
    "        accuracy_rn_pt_finetuned, \n",
    "        accuracy_vit, \n",
    "        accuracy_vit_finetuned\n",
    "    ],\n",
    "    'Time to Converge': [\n",
    "        timer_cnn.total_time, \n",
    "        timer_rn_scratch.total_time, \n",
    "        timer_rn_pt.total_time, \n",
    "        timer_rn_pt_finetuned.total_time, \n",
    "        timer_vit.total_time, \n",
    "        global_timer.total_time\n",
    "    ]\n",
    "})\n",
    "\n",
    "# ============================= COMMENT OUT ================================\n",
    "# BACKUP DATA in case the notebook has crashed.\n",
    "# backup_data = {\n",
    "#     'Model': ['Custom CNN', 'Custom ResNet', 'Pretrained ResNet', 'Fine-tuned ResNet', 'Pretrained ViT', 'Fine-tuned ViT'],\n",
    "#     'Test Accuracy': [0.9020, 0.8685, 0.8630, 0.9390, 0.9150, 0.7615],\n",
    "#     'Time to Converge': [10.58, 4.38, 8.95, 4.45, 36.41, 22.59]\n",
    "# }\n",
    "# all_models = pd.DataFrame(backup_data)\n",
    "# ==========================================================================\n",
    "\n",
    "\n",
    "# Sort by Accuracy\n",
    "all_models = all_models.sort_values(\"Test Accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### 3.? Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== COMPARISON PLOTTING ==============\n",
    "\n",
    "### COLORS & LOGIC\n",
    "\n",
    "# Define plot colors\n",
    "acc_color_light = \"#b3cde0\"  # Light Blue\n",
    "acc_color_strong = \"#005b96\" # Strong Blue\n",
    "\n",
    "time_color_light = \"#fbb4ae\" # Light Pink\n",
    "time_color_strong = \"#e31a1c\" # Strong Red\n",
    "\n",
    "# Find the best models\n",
    "best_acc_model = all_models.loc[all_models['Test Accuracy'].idxmax(), 'Model']\n",
    "best_time_model = all_models.loc[all_models['Time to Converge'].idxmin(), 'Model']\n",
    "\n",
    "# Map highlighted color to best models\n",
    "acc_palette = {model: acc_color_strong if model == best_acc_model else acc_color_light \n",
    "               for model in all_models['Model']}\n",
    "\n",
    "time_palette = {model: time_color_strong if model == best_time_model else time_color_light \n",
    "                for model in all_models['Model']}\n",
    "\n",
    "\n",
    "### PLOTTING\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")                 # Seaborn theme\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))  # 2 side-by-side plots\n",
    "\n",
    "# ============= Plot 1: Accuracy =============\n",
    "sns.barplot(\n",
    "    data=all_models,\n",
    "    x=\"Test Accuracy\",\n",
    "    y=\"Model\",\n",
    "    ax=axes[0],\n",
    "    hue=\"Model\",\n",
    "    palette=acc_palette,   # Pass custom accuracy color map\n",
    "    legend=False\n",
    ")\n",
    "axes[0].set_title(\"Model Accuracy\", fontsize=18, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Accuracy\", fontsize=14)\n",
    "axes[0].set_ylabel(\"\")\n",
    "axes[0].set_xlim(0, 1.1)\n",
    "axes[0].tick_params(axis='y', which='major', labelsize=16)\n",
    "\n",
    "# Add labels\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt=('%.2f'), padding=3, fontsize=16)\n",
    "\n",
    "\n",
    "# ============= Plot 2: Time =============\n",
    "sns.barplot(\n",
    "    data=all_models,\n",
    "    x=\"Time to Converge\",\n",
    "    y=\"Model\",\n",
    "    ax=axes[1],\n",
    "    hue=\"Model\",\n",
    "    palette=time_palette,  # Pass custom time color map\n",
    "    legend=False\n",
    ")\n",
    "axes[1].set_title(\"Time to Convergence\", fontsize=18, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Time (minutes)\", fontsize=14)\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[1].tick_params(axis='y', which='major', labelsize=16)\n",
    "axes[1].set_xlim(0, 40)\n",
    "\n",
    "# Add labels\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, fmt='%.1f', padding=3, fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "INTERPRET RESULTS\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "## 4. Conclusion and Discussion\n",
    "\n",
    "#### **Generalizability**\n",
    "\n",
    "The dataset consists of MRI images collected from multiple published studies, likely originating from different institutions and populations. Training on such heterogeneous data may enhance the model’s generalizability by reducing reliance on dataset- or scanner-specific features. However, since images from the same sources may appear in both training and evaluation sets, external validation on unseen institutions would be required to fully assess real-world generalizability.\n",
    "\n",
    "#### **Oppertunities for improving**\n",
    "Optimization of the models (specifically finetuning) and training/testing on more and diverse data.\n",
    "\n",
    "#### **Important take aways for practitioners and researchers**\n",
    "\n",
    "Based on our results the fine-tuned ResNet model is the best model. It is the most accurate and only slightly slower than the fastest model. The ResNet is sensible as convolutional architecture inherently understands shapes and edges in tumor. For the ViT model adding fine-tuning layers failed to improve performance of the model, likely due to overfitting, given our small data size and excessive (1 million extra) parameters.\n",
    "Moreover, it would be interesting to test hybrid models combining CNN's, capturing local patterns, and Vision Transformers, capturing global patterns. According to Takahashi et al. (2025) combining ResNet and ViT outperformed other existing models in classifying brain tumors in MRI.This might result in a model that is able to handle more diverse and complex image datasets making tumor classification more reliable in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "Shanto, M. N. I., Mubtasim, M. T., Rakshit, S. V., & Ullah, M. A. (2025). Enhanced Classification of Brain Tumors from MRI Scans Using a Hybrid CNN-Transformer Model. 2025 International Conference on Quantum Photonics, Artificial Intelligence, and Networking (QPAIN), 1–6. https://doi.org/10.1109/qpain66474.2025.11171896\n",
    "\n",
    "Takahashi, S., Sakaguchi, Y., Kouno, N., Takasawa, K., Ishizu, K., Akagi, Y., Aoyama, R.,      Teraya, N., Bolatkan, A., Shinkai, N., Machino, H., Kobayashi, K., Asada, K., Komatsu, M., Kaneko, S., Sugiyama, M., & Hamamoto, R. (2024). Comparison of Vision Transformers and Convolutional Neural Networks in Medical Image Analysis: A Systematic Review. Journal Of Medical Systems, 48(1), 84. https://doi.org/10.1007/s10916-024-02105-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "## 6. Division of labor\n",
    "\n",
    "**Charlotte de Vries:** Preprocessing data, Residual network models construction, training and evaluation, Conclusion and Discussion, Poster presentation design\n",
    "\n",
    "**Jiazhen Tang:** Preprocessing data, Data setup (data structure and over-sampling), Vision Transformer models construction, training and evaluation\n",
    "\n",
    "**Paulo Zirlis:** Preprocessing data, organising the Github environment, Introduction, Convolutional Neural Network model construction, training and evaluation + evaluation of all models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
