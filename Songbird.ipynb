{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0576f7e7",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "**Group:** Songbird  \n",
    "**Members:** Charlotte de Vries, Jiazhen Tang, Paulo Zirlis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49aa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup block (packages)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Setup OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a25a0",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "### 1.1 Objective*\n",
    "\n",
    "Apply different deep learning architectures to the visual classification task of identifying brain tumors in MRI images and compare them based on accuracy and time to converge.\n",
    "\n",
    "\n",
    "### 1.2 Neural Network Architectures\n",
    "\n",
    "We will implement and compare the following architectures:\n",
    "- Custom Convolutional Neural Network (CNN) with keras sequential\n",
    "- Custom Residual Network (ResNet)\n",
    "- Pre-trained Residual Network (ResNet50)\n",
    "- Pre-trained Residual Network (ResNet50) with fine-tuning\n",
    "- Pre-trained Visual Transformer (ViT)\n",
    "- Pre-trained Visual Transformer (ViT) with fine-tuning\n",
    "\n",
    "\n",
    "### 1.3 Dataset Description\n",
    "\n",
    "The dataset includes high-resolution CT and MRI images captured from multiple patients, with each image labeled with the corresponding tumor type (e.g., glioma, meningioma, etc.). For this project we will focus solely on the **MRI** images for simplicity. The dataset's creator collected these data from different sources to assist researchers and healthcare professionals in developing AI models for the automatic detection, classification, and segmentation of brain tumors.\n",
    "\n",
    "The images are divided as follows:\n",
    "- Healty images: 2000\n",
    "- Tumor images: 3000\n",
    "    - Meningioma: 1112\n",
    "    - Glioma: 672\n",
    "    - Pituitary: 629\n",
    "    - Tumor: 587\n",
    "- **Total of images:** 5000\n",
    "\n",
    "Source: [Brain tumor multimodal image (Kaggle)](https://www.kaggle.com/datasets/murtozalikhon/brain-tumor-multimodal-image-ct-and-mri/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d911f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3117d10",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### 2.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# 1. SETUP PATHS\n",
    "dataset_path = 'Data/Brain Tumor MRI images'\n",
    "\n",
    "print(f\"Checking contents of: {dataset_path}\")\n",
    "try:\n",
    "    items = os.listdir(dataset_path)\n",
    "    print(\"Found these items:\", items)\n",
    "except:\n",
    "    print(\"Error: The dataset_path does not exist.\")\n",
    "\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "\n",
    "# Get list of all folders in the main directory\n",
    "all_items = os.listdir(dataset_path)\n",
    "\n",
    "for item in all_items:\n",
    "    item_path = os.path.join(dataset_path, item)\n",
    "    \n",
    "    # We only care if it's a folder (directory)\n",
    "    if os.path.isdir(item_path):\n",
    "        \n",
    "        # --- CASE A: The 'Healthy' Folder ---\n",
    "        if 'healthy' in item.lower():\n",
    "            print(f\"Processing Healthy folder: {item}\")\n",
    "            for filename in os.listdir(item_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):\n",
    "                    filepaths.append(os.path.join(item_path, filename))\n",
    "                    labels.append('Healthy')\n",
    "                    \n",
    "        # --- CASE B: The 'Tumour' Folder (Anything that isn't Healthy) ---\n",
    "        else:\n",
    "            print(f\"Processing Tumour folder: {item}\")\n",
    "            for filename in os.listdir(item_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):\n",
    "                    full_path = os.path.join(item_path, filename)\n",
    "                    name_lower = filename.lower()\n",
    "                    \n",
    "                    # Determine Subtype based on filename\n",
    "                    if 'glioma' in name_lower:\n",
    "                        label = 'Glioma'\n",
    "                    elif 'meningioma' in name_lower:\n",
    "                        label = 'Meningioma'\n",
    "                    elif 'pituitary' in name_lower:\n",
    "                        label = 'Pituitary'\n",
    "                    else:\n",
    "                        label = 'Tumor (Unspecified)' \n",
    "                    \n",
    "                    filepaths.append(full_path)\n",
    "                    labels.append(label)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'filepath': filepaths, 'label': labels})\n",
    "\n",
    "# Check results\n",
    "print(f\"Total images found: {len(df)}\")\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358f234",
   "metadata": {},
   "source": [
    "### 2.2 Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da323497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split: 80% Train, 20% Test (using stratify to keep classes balanced)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42, stratify=df['label'])\n",
    "\n",
    "# 3. Split Train again to get Validation set (e.g. 10% of total)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.125, shuffle=True, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Val size:   {len(val_df)}\")\n",
    "print(f\"Test size:  {len(test_df)}\")\n",
    "\n",
    "# 4. Visualize to confirm labels are correct\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "# Get a random sample to check\n",
    "sample_df = df.sample(10)\n",
    "\n",
    "for i, (index, row) in enumerate(sample_df.iterrows()):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    img = mpimg.imread(row['filepath'])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"{row['label']}\\n{os.path.basename(row['filepath'])[:10]}...\", fontsize=9) # Show label + part of filename\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d54f74",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d75893",
   "metadata": {},
   "source": [
    "## 2. Neural Network Models\n",
    "\n",
    "### 2.1 Custom CNN\n",
    "\n",
    "The Convolutional Neural Network (CNN) model designed for this project consists of four convolutional blocks followed by a final block with pooling, dropout and fully connected layers. Each convolutional block has a convolution layer, batch normalization, activation function (ReLU) and max pooling. Early stopping was added to control for overfitting and underfitting. Batch normalization was used to improve training speed and stability. Dropout was included in the final block to further prevent overfitting. The model was compiled with the Adam optimizer, categorical cross-entropy loss function, and accuracy as the evaluation metric.\n",
    "\n",
    "<br>\n",
    "\n",
    "The architecture is as follows:\n",
    "\n",
    "**Input and Data Augmentation**\n",
    "- Input layer: shape (256, 256, 1)\n",
    "- Data Augmentation: Random rotations and horizontal flips\n",
    "\n",
    "**First Convolutional Block**\n",
    "- Conv2d layer: 32 filters, 3x3 kernel, stride of 1, same padding\n",
    "- Batch Normalization\n",
    "- ReLU Activation\n",
    "- MaxPooling2d layer: 2x2 pool size, stride of 2.\n",
    "\n",
    "**Other Convolutional Blocks**\n",
    "- same as the first block but with increasing number of filters (64, 128, 256)\n",
    "\n",
    "**Classifier Head**\n",
    "- Global Average Pooling layer\n",
    "- Dense layer: 64 units, ReLU activation\n",
    "- Dropout layer: 0.3 dropout rate\n",
    "- Dense layer: 5 units (nº of classes), Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0f0b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel: \"sequential\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
      "│ random_flip (\u001b[38;5;33mRandomFlip\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ random_rotation (\u001b[38;5;33mRandomRotation\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m320\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m256\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
      "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m16,448\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m325\u001b[0m │\n",
      "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m406,533\u001b[0m (1.55 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m405,573\u001b[0m (1.55 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
     ]
    }
   ],
   "source": [
    "### CNN Architecture\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Custom CNN\n",
    "CNN = keras.Sequential([\n",
    "    \n",
    "    # Input\n",
    "    layers.InputLayer(shape=[256, 256, 1]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    layers.RandomFlip(\"horizontal\"), # flip images horizontally\n",
    "    layers.RandomRotation(0.1),      # rotate images randomly by 10%\n",
    "\n",
    "\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Fourth Convolutional Block\n",
    "    layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Classifier Head\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=5, activation='softmax')  # 5 classes\n",
    "])\n",
    "\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and Evaluate CNN\n",
    "\n",
    "# Compile the model\n",
    "CNN.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 20,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = CNN.fit(\n",
    "    train,\n",
    "    validation_data = valid,\n",
    "    batch_size = 32,\n",
    "    epochs = 25,\n",
    "    callbacks = [early_stopping],\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "### ADD EVALUATION METRICS AND VISUALIZATIONS HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d289f3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215ed71",
   "metadata": {},
   "source": [
    "## 2.2 Pre-trained Residual Network\n",
    "Charlotte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f4d97",
   "metadata": {},
   "source": [
    "#### **ResNet without finetuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532149c5",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_c5_, ds_val_c5_ = image_dataset_from_directory(\n",
    "    '/tmp/BrainTumorDataset',\n",
    "    validation_split=0.2,\n",
    "    subset='both',\n",
    "    seed=42,\n",
    "    image_size=(224,224),\n",
    "    batch_size=32,\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52c68b",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "ds_train_c5 = ds_train_c5_.map(preprocess)\n",
    "ds_val_c5 = ds_val_c5_.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a79c1",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "def augment(image, label):\n",
    "    image = data_augmentation(image)\n",
    "    return image, label\n",
    "\n",
    "ds_train_c5 = ds_train_c5_.map(augment).map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96ca44",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "## pretrained base\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False   # Freeze weights\n",
    "\n",
    "## attach head\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af3f85",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model\n",
    "model.compile(\n",
    "    optimizer=Adam(1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_c5 = model.fit(\n",
    "    ds_train_c5,\n",
    "    validation_data=ds_val_c5,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c35e5e",
   "metadata": {},
   "source": [
    "#### Visualizing the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Loss\n",
    "# plt.plot(history_c5.history['loss'], label='train_loss')\n",
    "# plt.plot(history_c5.history['val_loss'], label='val_loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Accuracy\n",
    "# plt.plot(history_c5.history['accuracy'], label='train_accuracy')\n",
    "# plt.plot(history_c5.history['val_accuracy'], label='val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b433ed",
   "metadata": {},
   "source": [
    "#### **ResNet with finetuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc97247",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze the first 140 layers \"freeze\"\n",
    "for layer in base_model.layers[:140]:\n",
    "    layer.trainable = False\n",
    "\n",
    "## attach head\n",
    "model_finetuned = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a4bd2",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model_finetuned.compile(\n",
    "    optimizer=Adam(1e-5),    \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_c5_finetune = model_finetuned.fit(\n",
    "    ds_train_c5,\n",
    "    validation_data=ds_val_c5,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857aad0",
   "metadata": {},
   "source": [
    "#### Visualizing the loss and accuracy after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "# plt.plot(history_c5_finetune.history['loss'], label='train_loss')\n",
    "# plt.plot(history_c5_finetune.history['val_loss'], label='val_loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Accuracy\n",
    "# plt.plot(history_c5_finetune.history['accuracy'], label='train_accuracy')\n",
    "# plt.plot(history_c5_finetune.history['val_accuracy'], label='val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7929142",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b1086",
   "metadata": {},
   "source": [
    "## 2.3 Pre-trained Vision Transformer\n",
    "Jiazhen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d547757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ensure we know the number of classes (should be 5)\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# 2. Load the ViT Base Model (86M Parameters)\n",
    "model_id = \"google/vit-base-patch16-224\"\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    model_id, \n",
    "    num_labels=num_classes, \n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# 3. Define Optimizer and Loss (Using tf_keras to avoid version errors)\n",
    "# ViT requires a small learning rate (5e-5)\n",
    "optimizer = Adam(learning_rate=5e-5)\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 4. Compile the model\n",
    "# jit_compile=True will help speed up this heavy model on the GPU\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss, \n",
    "    metrics=['accuracy'],\n",
    "    jit_compile=True \n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded and compiled {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Generators with 'channels_first'\n",
    "# We add data_format='channels_first' to match the Hugging Face ViT requirements\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    data_format='channels_first'  # <--- THIS IS THE KEY FIX\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    data_format='channels_first'  # <--- THIS IS THE KEY FIX\n",
    ")\n",
    "\n",
    "# 2. Flow from DataFrame\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Generators recreated with Channels First format.\")\n",
    "\n",
    "# 3. Now run the training code again\n",
    "\n",
    "early_stopping = tf_keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10, \n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Backbone \n",
    "backbone = TFViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "backbone.trainable = True \n",
    "\n",
    "# 2. Create Input (Channels First)\n",
    "inputs = Input(shape=(3, 224, 224), name=\"input_image\")\n",
    "\n",
    "# 3. Backbone Inference\n",
    "x = backbone(inputs).last_hidden_state\n",
    "x = x[:, 0, :] # Extract CLS Token\n",
    "\n",
    "# 4. Custom Hidden Layers\n",
    "x = layers.Dense(512, activation='relu', name=\"hidden_layer_1\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(256, activation='relu', name=\"hidden_layer_2\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# 5. Output Layer\n",
    "outputs = layers.Dense(num_classes, name=\"prediction_head\")(x)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHANGE IS HERE: distinct variable name and model name\n",
    "# ---------------------------------------------------------\n",
    "fine_tuned_model = Model(inputs=inputs, outputs=outputs, name=\"ViT_With_Custom_Head\")\n",
    "\n",
    "# 6. Compile\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "loss = CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "fine_tuned_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Verify the name in the summary\n",
    "fine_tuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcacda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_custom = fine_tuned_model.fit(\n",
    "    train_generator,\n",
    "    epochs=10, \n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
