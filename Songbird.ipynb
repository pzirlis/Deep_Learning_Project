{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0576f7e7",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "**Group:** Songbird  \n",
    "**Members:** Charlotte de Vries, Jiazhen Tang, Paulo Zirlis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49aa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup block (packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a25a0",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd88b15",
   "metadata": {},
   "source": [
    "## 2. Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d54f74",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d75893",
   "metadata": {},
   "source": [
    "## 2. Neural Network Models\n",
    "### 2.1 Custom CNN\n",
    "\n",
    "The Convolutional Neural Network (CNN) model designed for this project consists of four convolutional blocks followed by a final block with pooling, dropout and fully connected layers. Each convolutional block has a convolution layer, batch normalization, activation function (ReLU) and max pooling. Early stopping was added to control for overfitting and underfitting. Batch normalization was used to improve training speed and stability. Dropout was included in the final block to further prevent overfitting.\n",
    "\n",
    "The architecture is as follows:\n",
    "\n",
    "**Input and Data Augmentation**\n",
    "- Input layer: shape (256, 256, 1)\n",
    "- Data Augmentation: Random rotations and horizontal flips\n",
    "\n",
    "**First Convolutional Block**\n",
    "- Conv2d layer: 32 filters, 3x3 kernel, stride of 1, same padding\n",
    "- Batch Normalization\n",
    "- ReLU Activation\n",
    "- MaxPooling2d layer: 2x2 pool size, stride of 2.\n",
    "\n",
    "**Other Convolutional Blocks**\n",
    "- same as the first block but with increasing number of filters (64, 128, 256)\n",
    "\n",
    "**Classifier Head**\n",
    "- Global Average Pooling layer\n",
    "- Dense layer: 64 units, ReLU activation\n",
    "- Dropout layer: 0.3 dropout rate\n",
    "- Dense layer: 5 units (nº of classes), Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0f0b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel: \"sequential_1\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
      "│ random_flip_3 (\u001b[38;5;33mRandomFlip\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ random_rotation_3 (\u001b[38;5;33mRandomRotation\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m320\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m256\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
      "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
      "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m16,448\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
      "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
      "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m325\u001b[0m │\n",
      "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
      "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m406,533\u001b[0m (1.55 MB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m405,573\u001b[0m (1.55 MB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
     ]
    }
   ],
   "source": [
    "### CNN Architecture\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Custom CNN\n",
    "CNN = keras.Sequential([\n",
    "    \n",
    "    # Input\n",
    "    layers.InputLayer(shape=[256, 256, 1]),\n",
    "    \n",
    "    # Data Augmentation\n",
    "    layers.RandomFlip(\"horizontal\"), # flip images horizontally\n",
    "    layers.RandomRotation(0.1),      # rotate images randomly by 10%\n",
    "\n",
    "\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(filters=128, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Fourth Convolutional Block\n",
    "    layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "    # Classifier Head\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(units=64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=5, activation='softmax')  # 5 classes\n",
    "])\n",
    "\n",
    "CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and Evaluate CNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Compile the model\n",
    "CNN.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta = 0.001,\n",
    "    patience = 20,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = CNN.fit(\n",
    "    train,\n",
    "    validation_data = valid,\n",
    "    batch_size = 32,\n",
    "    epochs = 25,\n",
    "    callbacks = [early_stopping],\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d289f3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215ed71",
   "metadata": {},
   "source": [
    "## 2.2 Pre-trained Residual Network\n",
    "Charlotte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for ResNet\n",
    "# hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b1086",
   "metadata": {},
   "source": [
    "## 2.3 Pre-trained Vision Transformer\n",
    "Jiazhen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for ViT"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
